{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of paragraphs: 33\n",
      "Processing paragraph 1\n",
      "Processing paragraph 2\n",
      "Processing paragraph 3\n",
      "Processing paragraph 4\n",
      "Processing paragraph 5\n",
      "Processing paragraph 6\n",
      "Processing paragraph 7\n",
      "Processing paragraph 8\n",
      "Processing paragraph 9\n",
      "Processing paragraph 10\n",
      "Processing paragraph 11\n",
      "Processing paragraph 12\n",
      "Processing paragraph 13\n",
      "Processing paragraph 14\n",
      "Processing paragraph 15\n",
      "Processing paragraph 16\n",
      "Processing paragraph 17\n",
      "Processing paragraph 18\n",
      "Processing paragraph 19\n",
      "Processing paragraph 20\n",
      "Processing paragraph 21\n",
      "Processing paragraph 22\n",
      "Processing paragraph 23\n",
      "Processing paragraph 24\n",
      "Processing paragraph 25\n",
      "Processing paragraph 26\n",
      "Processing paragraph 27\n",
      "Processing paragraph 28\n",
      "Processing paragraph 29\n",
      "Processing paragraph 30\n",
      "Processing paragraph 31\n",
      "Processing paragraph 32\n",
      "Processing paragraph 33\n",
      "Successfully written to file.\n"
     ]
    }
   ],
   "source": [
    "from general_functions.text_operations import tokenize_text\n",
    "from general_functions.file_operations import read_json_file\n",
    "from general_functions.file_operations import write_json_file\n",
    "from general_functions.emotion_analysis import get_emotion_scores_j_hartmann_bert\n",
    "import os\n",
    "\n",
    "input_text_file = \"../data/text.txt\"\n",
    "tokenized_file = \"../data/tokenized_text.json\"\n",
    "\n",
    "# Check if the tokenized file already exists\n",
    "if not os.path.exists(tokenized_file):\n",
    "    tokenize_text(input_text_file, tokenized_file)\n",
    "\n",
    "\n",
    "tokenized_data = read_json_file(tokenized_file)\n",
    "\n",
    "print(f\"Number of paragraphs: {len(tokenized_data)}\")\n",
    "\n",
    "data_with_emotion_scores = []\n",
    "for index, data in enumerate(tokenized_data):\n",
    "    print(f\"Processing paragraph {index + 1}\")\n",
    "    emotion_scores = get_emotion_scores_j_hartmann_bert(data[\"para_text\"])\n",
    "    data_with_emotion_scores.append(\n",
    "        {\n",
    "            \"label\": \"Paragraph \" + str(index + 1),\n",
    "            \"text\": data[\"para_text\"],\n",
    "            \"emotion_scores\": emotion_scores,\n",
    "        }\n",
    "    )\n",
    "write_json_file(\"../data/emotion_scores.json\", data_with_emotion_scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
