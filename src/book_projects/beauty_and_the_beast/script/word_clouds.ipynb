{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5904984a",
   "metadata": {},
   "source": [
    "Word Cloud - for one chapter or whole text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ddea2a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading input file...\n",
      "Tokenizing and filtering stopwords...\n",
      "Getting top 50 words...\n",
      "Translate words and format output process started...\n",
      "Processed 10 words...\n",
      "Processed 20 words...\n",
      "Processed 30 words...\n",
      "Processed 40 words...\n",
      "Processed 50 words...\n",
      "Writing to output file...\n",
      "Word cloud data saved to ../data/wcloud_perrault.json\n"
     ]
    }
   ],
   "source": [
    "from general_functions.chart_data.wordcloud_data import generate_wordcloud_data\n",
    "\n",
    "\n",
    "input_path = \"../data/perrault.txt\"\n",
    "output_path = \"../data/wcloud_perrault.json\"\n",
    "\n",
    "generate_wordcloud_data(input_path, output_path, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0eb6ff75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "def get_top_words(input_path: str, output_path: str, top_n: int):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    with open(input_path, \"r\") as f:\n",
    "        text = f.read().lower()\n",
    "\n",
    "    words = text.split()\n",
    "    word_freq = {}\n",
    "    for word in words:\n",
    "        word = word.translate(str.maketrans('', '', string.punctuation))  # remove punctuation\n",
    "        if word and word not in stop_words:  # check if word is not empty after removing punctuation\n",
    "            if word not in word_freq:\n",
    "                word_freq[word] = 1\n",
    "            else:\n",
    "                word_freq[word] += 1\n",
    "\n",
    "    sorted_word_freq = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)\n",
    "    top_words = sorted_word_freq[:top_n]\n",
    "\n",
    "    top_words_dict = {}\n",
    "    for word, freq in top_words:\n",
    "        top_words_dict[word] = freq\n",
    "\n",
    "    with open(output_path, \"w\") as f:\n",
    "        json.dump(top_words_dict, f, indent=4)\n",
    "\n",
    "\n",
    "input_path = \"../data/beaumont.txt\"\n",
    "output_path = \"../data/top_words_beaumont.json\"\n",
    "top_n = 20\n",
    "\n",
    "get_top_words(input_path, output_path, top_n)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2b3e227d",
   "metadata": {},
   "source": [
    "## Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dcddf7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sentiment chart \n",
    "\n",
    "#Required libraries \n",
    "import pandas as pd\n",
    "import spacy\n",
    "import json\n",
    "\n",
    "# Constants\n",
    "lexicon_path = \"../data/NRC-VAD-Lexicon.csv\"\n",
    "story_path = \"../data/beaumont.txt\"\n",
    "output = \"../data/sentiment_beaumont.json\"\n",
    "\n",
    "\n",
    "# Load the English model for SpaCy; used for tokenizing the story into sentences/words\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Reads the lexicon data; returns it as dataframe\n",
    "def load_lexicon(file_path):\n",
    "    return pd.read_csv(file_path, sep=\"\\t\")\n",
    "\n",
    "# Calculates a score for each word in a sentence based on valence and arousal values from lexicon\n",
    "# Final score for the sentence is the average of all word scores\n",
    "# If the word doesn't exist in lexicon or isn't alphanumeric, it's skipped.\n",
    "def get_negative_valence_arousal_score(sentence, lexicon_df):\n",
    "    doc = nlp(sentence)\n",
    "    scores = []\n",
    "    for token in doc:\n",
    "        word = token.text\n",
    "        if not word.isalnum():\n",
    "            continue\n",
    "        word_data = lexicon_df.loc[lexicon_df['Word'] == word]\n",
    "        if not word_data.empty:\n",
    "            valence = word_data['Valence'].values[0]\n",
    "            arousal = word_data['Arousal'].values[0]\n",
    "            negative_valence = 1 - valence\n",
    "            scores.append(negative_valence + arousal)\n",
    "    if scores:\n",
    "        return sum(scores) / len(scores)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Reads the story from file; splits it into chunks of 500 words; returns a list of chunks\n",
    "def read_and_split_story(file_path):\n",
    "    with open(file_path, \"r\") as file:\n",
    "        text = file.read()\n",
    "    words = text.split()\n",
    "    return [' '.join(words[i:i+500]) for i in range(0, len(words), 500)]\n",
    "\n",
    "# Analyzes each chunk in the story. Tokenizes each chunk into sentences\n",
    "# Computes sentiment for each sentence\n",
    "# Result for each chunk is the average sentiment score of its sentences \n",
    "# + sentence with the highest score\n",
    "def analyze_paragraphs(chunks, lexicon_df):\n",
    "    chunk_scores = {}\n",
    "    for index, chunk in enumerate(chunks):\n",
    "        sentence_scores = []\n",
    "        doc = nlp(chunk)\n",
    "        max_sent = \"\"\n",
    "        max_score = -100\n",
    "        for sent in doc.sents:\n",
    "            score = get_negative_valence_arousal_score(sent.text, lexicon_df)\n",
    "            if score and score > max_score:\n",
    "                max_score = score\n",
    "                max_sent = sent.text\n",
    "            sentence_scores.append(score)\n",
    "        sentence_scores = [score for score in sentence_scores if score is not None]\n",
    "        if not sentence_scores:\n",
    "            print(chunk)\n",
    "            continue\n",
    "        average_score = sum(sentence_scores) / len(sentence_scores)\n",
    "        chunk_scores[index * 500] = average_score\n",
    "    return chunk_scores\n",
    "\n",
    "# Saves a python dictionary as JSON file\n",
    "def write_to_json(data, output_path):\n",
    "    with open(output_path, \"w\") as json_file:\n",
    "        json.dump(data, json_file)\n",
    "\n",
    "# Main script \n",
    "\n",
    "# Load lexicon data\n",
    "lexicon_df = load_lexicon(lexicon_path)\n",
    "\n",
    "# Extract paragraphs from the story\n",
    "paragraphs = read_and_split_story(story_path)\n",
    "\n",
    "# Analyze the sentiment of paragraphs\n",
    "results = analyze_paragraphs(paragraphs, lexicon_df)\n",
    "\n",
    "\n",
    "# Save results to a JSON file\n",
    "write_to_json(results, output)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
